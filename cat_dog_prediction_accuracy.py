# -*- coding: utf-8 -*-
"""1. cat-dog-prediction-accuracy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3fk2PP1gekr4vUtegfCNq2Rsw2-7mK_

# Load Library
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import zipfile
import os
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator , load_img
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D , MaxPooling2D , Dropout , Flatten , Dense , Activation , BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import pathlib

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c dogs-vs-cats

!unzip dogs-vs-cats.zip
!unzip test1.zip
!unzip train.zip

"""# Load dataset"""

Image_width = 128
Image_height = 128
Image_size = (Image_width , Image_height)
Image_channel = 3

Image_rgb_size = (Image_width ,Image_height , 3 )

filenames = os.listdir('/content/train')
categories = []
for filename in filenames:
    category = filename.split('.')[0]
    if category == 'dog':
        categories.append(1)
    else:
        categories.append(0)
data_train = pd.DataFrame({'filename' : filenames , 'category' : categories})
data_train.head()

data_train['category'].value_counts()

sample_one = random.choice(filenames)
image = load_img('/content/train/{}'.format(sample_one))
plt.imshow(image)

data_train['category1'] = data_train['category'].apply(lambda x: 'cat' if x== 0 else 'dog')
data_train.head()

data_train.drop(['category'] , inplace = True , axis = 1)
data_train.rename(columns  = {'category1' : 'category'} , inplace =True)
data_train.head()

"""# Split data"""

train_df , val_df = train_test_split(data_train , test_size = 0.2 , random_state = 42)
train_df = train_df.reset_index(drop = True)
val_df = val_df.reset_index(drop = True)

train_df.shape , val_df.shape

batch_size  = 15
epochs = 7
total_train = train_df.shape[0]
total_validate = val_df.shape[0]

"""# Image Augmentation"""

train_dategen = ImageDataGenerator(rotation_range = 15 , 
                                  rescale = 1.0/255 ,
                                  shear_range = 0.1,
                                  zoom_range = 0.2 , 
                                  horizontal_flip = True , 
                                  width_shift_range = 0.1 , 
                                  height_shift_range = 0.1
                                  )

train_generator = train_dategen.flow_from_dataframe(
    train_df, 
    "/content/train", 
    x_col='filename',
    y_col='category',
    target_size=Image_size,
    class_mode='categorical',
    batch_size=batch_size,
    shuffle = True
)

validation_datagen = ImageDataGenerator(rescale = 1./255)
val_generator = validation_datagen.flow_from_dataframe(
    val_df, 
    "/content/train", 
    x_col='filename',
    y_col='category',
    target_size=Image_size,
    class_mode='categorical',
    batch_size=batch_size)

base_line = tf.keras.applications.resnet50.ResNet50(
    include_top=False,
    weights='imagenet',
    input_tensor=None,
    input_shape=Image_rgb_size)

x = base_line.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.7)(x)
predictions = Dense(2, activation= 'softmax')(x)
model = Model(inputs = base_line.input, outputs = predictions)

"""# Modelling data

"""

adam = Adam(learning_rate=0.0001)
model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

earlystop = EarlyStopping(patience = 10)
lr_reduc = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=2, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)
callbacks = [earlystop , lr_reduc]

with tf.device("/device:GPU:0"):
  history = model.fit(
      train_generator , 
      epochs = epochs , 
      validation_data=val_generator,
      validation_steps=total_validate//batch_size,
      steps_per_epoch=total_train//batch_size,
      callbacks=callbacks
      )

model.save_weights('model.h5')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

#test_filenames = os.listdir('/content/test1')
#data_test = pd.DataFrame({'filename' : test_filenames})
#nb_samples = test.shape[0]

#data_test.head()

#test_gen = ImageDataGenerator(rescale=1./255)
#test_generator = test_gen.flow_from_dataframe(
    #test , 
    #'/content/test1' ,
    #x_col='filename',
    #y_col=None,
    #class_mode=None,
    #target_size=Image_size,
    #batch_size=batch_size,
    #shuffle=False
    #)

#predict = model.predict(test_generator , steps = np.ceil(nb_samples/batch_size))
#data_test['category'] = np.argmax(predict  , axis= -1)
#data_test.head()

#image = load_img('/content/test1/{}'.format('1366.jpg'))
#plt.imshow(image)

#sns.countplot(test['category'])

"""# Deploy model"""

# Save model into SavedModel format
export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)
 
# Convert SavedModel to vegs.tflite
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()
 
tflite_model_file = pathlib.Path('vegs.tflite')
tflite_model_file.write_bytes(tflite_model)

# Commented out IPython magic to ensure Python compatibility.
# Save the entire model to a HDF5 file.
# The '.h5' extension indicates that the model should be saved to HDF5.
model.save('my_model.pb') 
# %cd